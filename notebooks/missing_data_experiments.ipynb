{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring with Missing Data Analysis\n",
    "\n",
    "This notebook demonstrates the analysis of credit scoring data with a focus on handling Missing Not At Random (MNAR) data. We'll compare different methods for handling missing data and evaluate their impact on model performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1.-Setup-and-Data-Loading)\n",
    "2. [Data Preprocessing](#2.-Data-Preprocessing)\n",
    "3. [Missing Data Analysis](#3.-Missing-Data-Analysis)\n",
    "4. [Handling Missing Data](#4.-Handling-Missing-Data)\n",
    "5. [Model Training](#5.-Model-Training)\n",
    "6. [Model Evaluation](#6.-Model-Evaluation)\n",
    "7. [Results Comparison](#7.-Results-Comparison)\n",
    "8. [Conclusions](#8.-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom modules\n",
    "from src.missing_data_handler import MissingDataHandler\n",
    "from src.model import CreditScoringModel\n",
    "from src.evaluation import ModelEvaluator\n",
    "from src.utils import preprocess_data, plot_missingness, plot_feature_distributions, create_correlation_matrix, split_data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/raw/credit_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Let's examine our data and perform initial preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "processed_df = preprocess_data(df)\n",
    "print(f\"Processed dataset shape: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Data Analysis\n",
    "\n",
    "Let's analyze the patterns of missing data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plot_missingness(df)\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(processed_df)\n",
    "\n",
    "# Create correlation matrix\n",
    "create_correlation_matrix(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data\n",
    "\n",
    "We'll apply different methods to handle missing data and create multiple versions of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize missing data handler\n",
    "handler = MissingDataHandler()\n",
    "\n",
    "# Apply different missing data handling methods\n",
    "df_mean = handler.mean_imputation(processed_df.copy(), 'target')\n",
    "df_heckman = handler.heckman_correction(processed_df.copy(), 'target', 'income')\n",
    "df_basl = handler.basl_method(processed_df.copy(), 'target')\n",
    "\n",
    "# Store datasets in a dictionary\n",
    "datasets = {\n",
    "    'mean_imputation': df_mean,\n",
    "    'heckman_correction': df_heckman,\n",
    "    'basl_method': df_basl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now we'll train models using each version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and evaluator\n",
    "model = CreditScoringModel()\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models for each dataset\n",
    "for method_name, dataset in datasets.items():\n",
    "    print(f\"\\nProcessing {method_name}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = dataset.drop('target', axis=1)\n",
    "    y = dataset['target']\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "    # Train model\n",
    "    model.train(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    results[method_name] = evaluator.evaluate_model(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "evaluator.compare_models(results)\n",
    "\n",
    "# Print detailed results\n",
    "for method_name, metrics in results.items():\n",
    "    print(f\"\\nResults for {method_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison\n",
    "\n",
    "Let's analyze the differences between the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "evaluator.compare_models(results, metrics=metrics_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "Based on our analysis:\n",
    "\n",
    "1. **Method Comparison**:\n",
    "   - [Fill in observations about which method performed best]\n",
    "   - [Note any interesting patterns in the results]\n",
    "\n",
    "2. **Practical Implications**:\n",
    "   - [Discuss what these results mean for credit scoring]\n",
    "   - [Note any limitations or areas for future research]\n",
    "\n",
    "3. **Recommendations**:\n",
    "   - [Provide specific recommendations based on the results]\n",
    "   - [Suggest best practices for handling missing data in credit scoring]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
