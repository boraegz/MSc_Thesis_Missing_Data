{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring with Missing Data Analysis\n",
    " \n",
    "This notebook demonstrates the analysis of credit scoring data with a focus on handling Missing Not At Random (MNAR) data. We'll compare different methods for handling missing data and evaluate their impact on model performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1.-Setup-and-Data-Loading)\n",
    "    - 1.1 [Libraries](#1.1-Libraries) \n",
    "    - 1.2 [Data Simulation](#1.2-Data-Simulation)\n",
    "2. [Data Preprocessing](#2.-Data-Preprocessing)\n",
    "    - 2.1 [Introducing Missingness](#2.1-Introducing-Missingness) \n",
    "    - 2.2 [Data Exploration](#2.2-Data-Exploration)\n",
    "3. [Missing Data Analysis](#3.-Missing-Data-Analysis)\n",
    "4. [Handling Missing Data](#4.-Handling-Missing-Data)\n",
    "    - 4.1 [Functioned Handling of Missing Data --Work in progress](#4.1-Functioned-Handling-of-Missing Data) \n",
    "    - 4.2 [Imputation by MICE](#4.2-Imputation-by-MICE)\n",
    "    - 4.3 [No Imputation at All](#4.2-No-Imputation-at-All)\n",
    "5. [Model Part](#5.-Model-Part)\n",
    "    - 5.1 [Loading Data and Initiating Model Class](#5.1-Loading-Data-and-Initiating-Model-Class) \n",
    "    - 5.2 [Train-Test Split](#5.2-Train-Test-Split)\n",
    "    - 5.3 [Model Training and Evaluation](#5.3-Model-Training-and-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Import our custom modules\n",
    "from missing_data_handler import MissingDataHandler\n",
    "from model import CreditScoringModel\n",
    "from utils import preprocess_data, plot_missingness, plot_feature_distributions, create_correlation_matrix, split_data\n",
    "from data_simulator import DataSimulator\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Simulation\n",
    "\n",
    "Simulating a sample dataset with 10 variables and 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DataSimulator class\n",
    "simulator = DataSimulator(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Generate synthetic credit data\n",
    "data = simulator.simulate_credit_data()\n",
    "print(\"\\nOriginal Data Shape:\", data.shape)\n",
    "print(\"\\nMissing values before introducing missingness:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Let's introduce different types of missingess mechanisms and examine our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introducing Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce different types of missingness\n",
    "data_mcar = simulator.introduce_missingness(data.copy(), mechanism='MCAR', missing_proportion=0.2, missing_col='feature_0')\n",
    "data_mar = simulator.introduce_missingness(data.copy(), mechanism='MAR', missing_proportion=0.2, missing_col='feature_0')\n",
    "data_mnar = simulator.introduce_missingness(data.copy(), mechanism='MNAR', missing_proportion=0.2, missing_col='target')\n",
    "\n",
    "# Display missing value counts\n",
    "print(\"\\nMissing values in MCAR dataset:\\n\", data_mcar.isnull().sum())\n",
    "print(\"\\nMissing values in MAR dataset:\\n\", data_mar.isnull().sum())\n",
    "print(\"\\nMissing values in MNAR dataset:\\n\", data_mnar.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the MCAR dataset\n",
    "print(\"Dataset Info for MCAR:\")\n",
    "data_mcar.info()\n",
    "\n",
    "print(\"\\nSummary Statistics for MCAR:\")\n",
    "data_mcar.describe()\n",
    "\n",
    "# Display basic information about the MAR dataset\n",
    "print(\"Dataset Info for MAR:\")\n",
    "data_mar.info()\n",
    "\n",
    "print(\"\\nSummary Statistics for MAR:\")\n",
    "data_mar.describe()\n",
    "\n",
    "# Display basic information about the MNAR dataset\n",
    "print(\"Dataset Info for MNAR:\")\n",
    "data_mnar.info()\n",
    "\n",
    "print(\"\\nSummary Statistics for MNAR:\")\n",
    "data_mnar.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Data Analysis\n",
    "\n",
    "Let's analyze the patterns of missing data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plot_missingness(data_mcar, \"MCAR Dataset\")\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(data_mcar, \"MCAR Dataset\")\n",
    "\n",
    "# Create correlation matrix\n",
    "create_correlation_matrix(data_mcar, \"MCAR Dataset\")\n",
    "\n",
    "# Visualize missing data patterns\n",
    "plot_missingness(data_mar, \"MAR Dataset\")\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(data_mar, \"MAR Dataset\")\n",
    "\n",
    "# Create correlation matrix\n",
    "create_correlation_matrix(data_mar, \"MAR Dataset\")\n",
    "\n",
    "# Visualize missing data patterns\n",
    "plot_missingness(data_mnar, \"MNAR Dataset\")\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(data_mnar, \"MNAR Dataset\")\n",
    "\n",
    "# Create correlation matrix\n",
    "create_correlation_matrix(data_mnar, \"MNAR Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data\n",
    "\n",
    "We'll apply different methods to handle missing data and create multiple versions of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Functioned Handling of Missing Data --Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize missing data handler\n",
    "# handler = MissingDataHandler()\n",
    "\n",
    "# # Apply different missing data handling methods\n",
    "# df_mean = handler.mean_imputation(data_mnar.copy(), 'target')\n",
    "# df_heckman = handler.heckman_correction(data_mnar.copy(), 'target', 'feature_0')\n",
    "# df_basl = handler.basl_method(data_mnar.copy(), 'target')\n",
    "\n",
    "\n",
    "# # Store datasets in a dictionary\n",
    "# datasets = {\n",
    "#     'mean_imputation': df_mean,\n",
    "#     'heckman_correction': df_heckman,\n",
    "#     'basl_method': df_basl\n",
    "# }\n",
    "\n",
    "# # 1. Compare target distributions\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# for idx, (method, df) in enumerate(datasets.items(), 1):\n",
    "#     plt.subplot(1, 3, idx)\n",
    "#     sns.countplot(data=df, x='target')\n",
    "#     plt.title(f'Target Distribution - {method}')\n",
    "#     plt.xlabel('Target')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     # Add percentage labels\n",
    "#     total = len(df)\n",
    "#     for p in plt.gca().patches:\n",
    "#         percentage = f'{100 * p.get_height()/total:.1f}%'\n",
    "#         plt.gca().annotate(percentage, (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "#                           ha='center', va='bottom')\n",
    "    \n",
    "#     # Force x-axis to show only 0 and 1\n",
    "#     plt.gca().set_xticks([0, 1])\n",
    "#     plt.gca().set_xticklabels(['0', '1'])\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 2. Compare feature distributions for each method\n",
    "# for method, df in datasets.items():\n",
    "#     print(f\"\\nFeature Distributions for {method}:\")\n",
    "#     plot_feature_distributions(df, f\"{method} - MNAR Dataset\")\n",
    "\n",
    "# # 3. Compare correlation matrices\n",
    "# for method, df in datasets.items():\n",
    "#     print(f\"\\nCorrelation Matrix for {method}:\")\n",
    "#     create_correlation_matrix(df, f\"{method} - MNAR Dataset\")\n",
    "\n",
    "# # 4. Print basic statistics for each method\n",
    "# for method, df in datasets.items():\n",
    "#     print(f\"\\nBasic Statistics for {method}:\")\n",
    "#     print(df.describe())\n",
    "\n",
    "# # 5. Compare missing values\n",
    "# for method, df in datasets.items():\n",
    "#     print(f\"\\nMissing Values for {method}:\")\n",
    "#     print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Imputation by MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN (to avoid issues)\n",
    "data_mnar['target'] = data_mnar['target'].replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Separate features and target\n",
    "features = data_mnar.drop(columns=['target'])\n",
    "target = data_mnar['target']\n",
    "\n",
    "# Step 1: Impute only the features (excluding target)\n",
    "imputer_features = IterativeImputer(random_state=42, max_iter=50)\n",
    "features_imputed = imputer_features.fit_transform(features)\n",
    "features_imputed_df = pd.DataFrame(features_imputed, columns=features.columns)\n",
    "\n",
    "# Step 2: Impute the binary target separately using Logistic Regression\n",
    "target_imputer = IterativeImputer(\n",
    "    estimator=LogisticRegression(class_weight=\"balanced\"), \n",
    "    random_state=42,\n",
    "    max_iter=50,\n",
    "    sample_posterior=False,  \n",
    "    min_value=0,\n",
    "    max_value=1\n",
    ")\n",
    "\n",
    "target_reshaped = target.values.reshape(-1, 1)  # Ensure correct shape for imputation\n",
    "target_imputed_nd = target_imputer.fit_transform(target_reshaped)\n",
    "\n",
    "# Convert imputed target back to binary\n",
    "target_imputed = np.round(target_imputed_nd).astype(int).flatten()\n",
    "\n",
    "# Combine imputed features and target\n",
    "data_imputed = features_imputed_df.copy()\n",
    "data_imputed['target'] = target_imputed\n",
    "\n",
    "# Save the final dataset\n",
    "data_imputed.to_csv(\"data_mnar_mice_imputed.csv\", index=False)\n",
    "\n",
    "# Verify imputation\n",
    "print(\"Imputation completed. Target variable distribution before MICE:\")\n",
    "print(target.value_counts(dropna=False))  # Include NaN counts\n",
    "print(\"Target variable distribution after MICE:\")\n",
    "print(data_imputed['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 No Imputation at All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_missing = data_mnar.dropna()\n",
    "\n",
    "# Step 3: Separate features and target\n",
    "features_no_missing = data_no_missing.drop(columns=['target'])\n",
    "target_no_missing = data_no_missing['target']\n",
    "\n",
    "# Show a summary of the data after removing missing values\n",
    "print(\"Data after removing rows with missing values:\")\n",
    "print(data_no_missing.info())\n",
    "\n",
    "# Optionally, you can save the cleaned dataset\n",
    "data_no_missing.to_csv(\"data_no_missing.csv\", index=False)\n",
    "\n",
    "# Verify that there are no missing values\n",
    "print(\"Target variable distribution after removing missing rows:\")\n",
    "print(target_no_missing.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Part\n",
    "\n",
    "Now we'll train models using each version of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading Data and Initiating Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your datasets\n",
    "data_mnar_mice = pd.read_csv(\"data_mnar_mice_imputed.csv\")\n",
    "data_mnar_no_missing = pd.read_csv(\"data_no_missing.csv\")  # Dataset without missing values\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model_mnar_mice = CreditScoringModel(random_state=42, class_weight='balanced')\n",
    "model_mnar_no_missing = CreditScoringModel(random_state=42, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Show a summary of the data\n",
    "print(data_mnar_mice.info())\n",
    "print(data_mnar_no_missing.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'target'\n",
    "\n",
    "\n",
    "# Prepare the data (train-test split)\n",
    "X_train_mnar, X_test_mnar, y_train_mnar, y_test_mnar = model_mnar_mice.prepare_data(data_mnar_mice, target=target_column)\n",
    "X_train_no_missing, X_test_no_missing, y_train_no_missing, y_test_no_missing = model_mnar_no_missing.prepare_data(data_mnar_no_missing, target=target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model with MICE imputed data\n",
    "print(\"Evaluating model with MICE imputation...\")\n",
    "results_mnar = model_mnar_mice.evaluate_model(X_train_mnar, X_test_mnar, y_train_mnar, y_test_mnar)\n",
    "\n",
    "# Train and evaluate the model with data that had missing values removed\n",
    "print(\"Evaluating model with no missing data...\")\n",
    "results_no_missing = model_mnar_no_missing.evaluate_model(X_train_no_missing, X_test_no_missing, y_train_no_missing, y_test_no_missing)\n",
    "\n",
    "# Compare results\n",
    "performance_data = {\n",
    "    \"Imputation Method\": [\"MICE Imputation\", \"No Missing Data\"],\n",
    "    \"Accuracy\": [results_mnar['accuracy'], results_no_missing['accuracy']],\n",
    "    \"Macro Avg Precision\": [\n",
    "        results_mnar['classification_report']['macro avg']['precision'],\n",
    "        results_no_missing['classification_report']['macro avg']['precision']\n",
    "    ],\n",
    "    \"Macro Avg Recall\": [\n",
    "        results_mnar['classification_report']['macro avg']['recall'],\n",
    "        results_no_missing['classification_report']['macro avg']['recall']\n",
    "    ],\n",
    "    \"Macro Avg F1-Score\": [\n",
    "        results_mnar['classification_report']['macro avg']['f1-score'],\n",
    "        results_no_missing['classification_report']['macro avg']['f1-score']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(performance_data)\n",
    "\n",
    "print(\"\\nComparison of Model Performance:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
