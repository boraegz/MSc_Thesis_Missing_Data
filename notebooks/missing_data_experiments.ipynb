{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring with Missing Data Analysis\n",
    "\n",
    "This notebook demonstrates the analysis of credit scoring data with a focus on handling Missing Not At Random (MNAR) data. We'll compare different methods for handling missing data and evaluate their impact on model performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1.-Setup-and-Data-Loading)\n",
    "2. [Data Preprocessing](#2.-Data-Preprocessing)\n",
    "3. [Missing Data Analysis](#3.-Missing-Data-Analysis)\n",
    "4. [Handling Missing Data](#4.-Handling-Missing-Data)\n",
    "5. [Model Training](#5.-Model-Training)\n",
    "6. [Model Evaluation](#6.-Model-Evaluation)\n",
    "7. [Results Comparison](#7.-Results-Comparison)\n",
    "8. [Conclusions](#8.-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n0/3n_f47sj56dbj3q_xx6vn93w0000gp/T/ipykernel_46367/1090670762.py:22: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_accepted_gz = pd.read_csv(file_path_accepted_gz, compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2260701, 151)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Import our custom modules\n",
    "# from src.missing_data_handler import MissingDataHandler\n",
    "# from src.model import CreditScoringModel\n",
    "# from src.evaluation import ModelEvaluator\n",
    "# from src.utils import preprocess_data, plot_missingness, plot_feature_distributions, create_correlation_matrix, split_data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Read a .gz file\n",
    "file_path_accepted_gz = '/Users/boraeguz/MSc_Thesis_Missing_Data/data/raw/accepted_2007_to_2018Q4.csv.gz'\n",
    "df_accepted_gz = pd.read_csv(file_path_accepted_gz, compression='gzip')\n",
    "print(f\"Dataset shape: {df_accepted_gz.shape}\")\n",
    "\n",
    "file_path_rejected_gz = '/Users/boraeguz/MSc_Thesis_Missing_Data/data/raw/rejected_2007_to_2018Q4.csv.gz'\n",
    "df_rejected_gz = pd.read_csv(file_path_rejected_gz, compression='gzip')\n",
    "\n",
    "# file_path_accepted_csv = '/Users/boraeguz/MSc_Thesis_Missing_Data/data/raw/accepted_2007_to_2018Q4.csv'\n",
    "# df_accepted_csv = pd.read_csv(file_path_accepted_csv)   \n",
    "\n",
    "# file_path_rejected_csv = '/Users/boraeguz/MSc_Thesis_Missing_Data/data/raw/rejected_2007_to_2018Q4.csv'\n",
    "# df_rejected_csv = pd.read_csv(file_path_rejected_csv)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Let's examine our data and perform initial preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27648741 entries, 0 to 27648740\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   Amount Requested      float64\n",
      " 1   Application Date      object \n",
      " 2   Loan Title            object \n",
      " 3   Risk_Score            float64\n",
      " 4   Debt-To-Income Ratio  object \n",
      " 5   Zip Code              object \n",
      " 6   State                 object \n",
      " 7   Employment Length     object \n",
      " 8   Policy Code           float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 1.9+ GB\n",
      "\n",
      "Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount Requested</th>\n",
       "      <th>Risk_Score</th>\n",
       "      <th>Policy Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.764874e+07</td>\n",
       "      <td>9.151111e+06</td>\n",
       "      <td>2.764782e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.313324e+04</td>\n",
       "      <td>6.281721e+02</td>\n",
       "      <td>6.375113e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.500964e+04</td>\n",
       "      <td>8.993679e+01</td>\n",
       "      <td>1.127368e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.800000e+03</td>\n",
       "      <td>5.910000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>6.370000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>6.750000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.400000e+06</td>\n",
       "      <td>9.900000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amount Requested    Risk_Score   Policy Code\n",
       "count      2.764874e+07  9.151111e+06  2.764782e+07\n",
       "mean       1.313324e+04  6.281721e+02  6.375113e-03\n",
       "std        1.500964e+04  8.993679e+01  1.127368e-01\n",
       "min        0.000000e+00  0.000000e+00  0.000000e+00\n",
       "25%        4.800000e+03  5.910000e+02  0.000000e+00\n",
       "50%        1.000000e+04  6.370000e+02  0.000000e+00\n",
       "75%        2.000000e+04  6.750000e+02  0.000000e+00\n",
       "max        1.400000e+06  9.900000e+02  2.000000e+00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df_rejected_gz.info()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "df_rejected_gz.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m(df_rejected_gz)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "processed_df = preprocess_data(df_rejected_gz)\n",
    "print(f\"Processed dataset shape: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Data Analysis\n",
    "\n",
    "Let's analyze the patterns of missing data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_missingness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize missing data patterns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_missingness\u001b[49m(df_accepted_gz)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot feature distributions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plot_feature_distributions(df_rejected_gz)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_missingness' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize missing data patterns\n",
    "plot_missingness(processed_df)\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(processed_df)\n",
    "\n",
    "# Create correlation matrix\n",
    "create_correlation_matrix(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data\n",
    "\n",
    "We'll apply different methods to handle missing data and create multiple versions of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize missing data handler\n",
    "handler = MissingDataHandler()\n",
    "\n",
    "# Apply different missing data handling methods\n",
    "df_mean = handler.mean_imputation(processed_df.copy(), 'target')\n",
    "df_heckman = handler.heckman_correction(processed_df.copy(), 'target', 'income')\n",
    "df_basl = handler.basl_method(processed_df.copy(), 'target')\n",
    "\n",
    "# Store datasets in a dictionary\n",
    "datasets = {\n",
    "    'mean_imputation': df_mean,\n",
    "    'heckman_correction': df_heckman,\n",
    "    'basl_method': df_basl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now we'll train models using each version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and evaluator\n",
    "model = CreditScoringModel()\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models for each dataset\n",
    "for method_name, dataset in datasets.items():\n",
    "    print(f\"\\nProcessing {method_name}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = dataset.drop('target', axis=1)\n",
    "    y = dataset['target']\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "    # Train model\n",
    "    model.train(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    results[method_name] = evaluator.evaluate_model(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "evaluator.compare_models(results)\n",
    "\n",
    "# Print detailed results\n",
    "for method_name, metrics in results.items():\n",
    "    print(f\"\\nResults for {method_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison\n",
    "\n",
    "Let's analyze the differences between the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "evaluator.compare_models(results, metrics=metrics_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "Based on our analysis:\n",
    "\n",
    "1. **Method Comparison**:\n",
    "   - [Fill in observations about which method performed best]\n",
    "   - [Note any interesting patterns in the results]\n",
    "\n",
    "2. **Practical Implications**:\n",
    "   - [Discuss what these results mean for credit scoring]\n",
    "   - [Note any limitations or areas for future research]\n",
    "\n",
    "3. **Recommendations**:\n",
    "   - [Provide specific recommendations based on the results]\n",
    "   - [Suggest best practices for handling missing data in credit scoring]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
