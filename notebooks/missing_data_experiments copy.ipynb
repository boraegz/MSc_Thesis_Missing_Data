{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Analysis on Missing Data Handling in Credit Scoring\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, we will explore different ways to handle missing data in credit scoring datasets. The primary objectives are:\n",
    "\n",
    "1. Simulate a synthetic credit dataset.\n",
    "2. Introduce missingness into the dataset using various missing data mechanisms (MCAR, MAR, MNAR).\n",
    "3. Visualize the missingness patterns in the data.\n",
    "4. Handle missing data using different imputation methods, Heckman correction, and BASL (Bias-Aware Self Learning).\n",
    "5. Split the data for training and testing.\n",
    "6. Train a machine learning model.\n",
    "7. Evaluate the model using various performance metrics.\n",
    "8. Conduct experiments to compare the performance of different missing data handling techniques.\n",
    "9. Visualize the results and draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used\n",
    "\n",
    "We will use the following libraries in this notebook:\n",
    "\n",
    "1. **pandas**: For data manipulation.\n",
    "2. **numpy**: For numerical operations and data generation.\n",
    "3. **matplotlib** and **seaborn**: For visualization.\n",
    "4. **sklearn**: For machine learning and evaluation metrics.\n",
    "5. **statsmodels**: For the Heckman correction model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, accuracy_score, brier_score_loss\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_simulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataSimulator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing_data_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MissingDataHandler\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mheckman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HeckmanCorrection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss\n",
    "from src.data_simulator import DataSimulator\n",
    "from src.missing_data_handler import MissingDataHandler\n",
    "from src.heckman import HeckmanCorrection\n",
    "from src.basl import BASLCorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simulating Data\n",
    "\n",
    "We will create a synthetic dataset of 1000 samples with features like age, income, debt, and a target variable representing the repayment label (0 = default, 1 = no default).\n",
    "\n",
    "Let's create this synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataSimulator class\n",
    "simulator = DataSimulator(num_samples=1000)\n",
    "\n",
    "# Generate synthetic data\n",
    "data = simulator.generate_data()\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introducing Missingness\n",
    "\n",
    "We will introduce missingness in the `Repayment_Label` column based on three different mechanisms:\n",
    "- **MCAR**: Missing Completely at Random\n",
    "- **MAR**: Missing at Random\n",
    "- **MNAR**: Missing Not at Random\n",
    "\n",
    "We'll apply these missingness mechanisms one by one to simulate different real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce MCAR missingness\n",
    "data_mcar = simulator.introduce_missingness(data, missingness_type=\"MCAR\", missing_rate=0.2)\n",
    "\n",
    "# Introduce MAR missingness\n",
    "data_mar = simulator.introduce_missingness(data, missingness_type=\"MAR\", missing_rate=0.2)\n",
    "\n",
    "# Introduce MNAR missingness\n",
    "data_mnar = simulator.introduce_missingness(data, missingness_type=\"MNAR\", missing_rate=0.2)\n",
    "\n",
    "# Display the data with MCAR missingness\n",
    "data_mcar.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizing Missing Data\n",
    "\n",
    "Let's visualize the missing data patterns using a heatmap, which will help us understand where and how the missing values are distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the missingness heatmap\n",
    "def plot_missing_data_heatmap(data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title(\"Missing Data Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize missing data for the MCAR dataset\n",
    "plot_missing_data_heatmap(data_mcar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handling Missing Data\n",
    "\n",
    "We will apply three different methods to handle the missing data:\n",
    "1. **Imputation**: Using mean imputation.\n",
    "2. **Heckman Correction**: To handle MNAR missingness.\n",
    "3. **BASL**: A more advanced approach that combines bias correction with self-learning.\n",
    "\n",
    "We'll handle missing data for all three datasets (MCAR, MAR, MNAR) using these methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the handlers\n",
    "missing_data_handler = MissingDataHandler()\n",
    "heckman_correction = HeckmanCorrection()\n",
    "basl_correction = BASLCorrection()\n",
    "\n",
    "# Impute missing data (MCAR example)\n",
    "data_imputed_mcar = missing_data_handler.impute_missing_data(data_mcar)\n",
    "\n",
    "# Apply Heckman correction (MNAR example)\n",
    "data_heckman_mnar = heckman_correction.apply_heckman(data_mnar)\n",
    "\n",
    "# Apply BASL correction (MNAR example)\n",
    "data_basl_mnar = basl_correction.apply_basl(data_mnar)\n",
    "\n",
    "# Display the processed data\n",
    "data_imputed_mcar.head(), data_heckman_mnar.head(), data_basl_mnar.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the Data\n",
    "\n",
    "We'll split the data into training and testing sets. We'll use 80% of the data for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable for the MCAR dataset\n",
    "X_mcar = data_imputed_mcar.drop(columns='Repayment_Label')\n",
    "y_mcar = data_imputed_mcar['Repayment_Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mcar, y_mcar, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the resulting datasets\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training a Model\n",
    "\n",
    "We'll train a Random Forest classifier on the training data to predict the repayment label (default or no default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Check the training accuracy\n",
    "train_accuracy = rf_model.score(X_train, y_train)\n",
    "train_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Importances\n",
    "\n",
    "We will extract and visualize the feature importances from the trained Random Forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=X_train.columns, y=feature_importances)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation\n",
    "\n",
    "We will evaluate the performance of the trained model using several metrics:\n",
    "1. **AUC (Area Under the Curve)**\n",
    "2. **Brier Score**\n",
    "3. **Accuracy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "brier_score = brier_score_loss(y_test, y_pred_prob)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display the metrics\n",
    "auc_score, brier_score, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Experiments and Visualizing Results\n",
    "\n",
    "Now, let's compare the performance of the different missing data handling techniques (Imputation, Heckman, and BASL) using the AUC, Brier Score, and Accuracy metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the models with different missing data handling methods\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, method_name=\"Imputation\"):\n",
    "    if method_name == \"Imputation\":\n",
    "        # Handle missing data using imputation (MCAR)\n",
    "        data = missing_data_handler.impute_missing_data(data_mcar)\n",
    "    elif method_name == \"Heckman\":\n",
    "        # Handle missing data using Heckman correction (MNAR)\n",
    "        data = heckman_correction.apply_heckman(data_mnar)\n",
    "    elif method_name == \"BASL\":\n",
    "        # Handle missing data using BASL correction (MNAR)\n",
    "        data = basl_correction.apply_basl(data_mnar)\n",
    "    \n",
    "    # Split the data again for model training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns='Repayment_Label'), data['Repayment_Label'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "    brier_score = brier_score_loss(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "    \n",
    "    return auc_score, brier_score, accuracy\n",
    "\n",
    "# Evaluate using different methods\n",
    "results = {\n",
    "    \"Imputation\": evaluate_model(X_train, X_test, y_train, y_test, \"Imputation\"),\n",
    "    \"Heckman\": evaluate_model(X_train, X_test, y_train, y_test, \"Heckman\"),\n",
    "    \"BASL\": evaluate_model(X_train, X_test, y_train, y_test, \"BASL\")\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results, index=[\"AUC\", \"Brier Score\", \"Accuracy\"])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusion\n",
    "\n",
    "In this thesis, we explored various methods for handling missing data in credit scoring datasets. We simulated data with different missingness mechanisms (MCAR, MAR, MNAR), applied several imputation and correction techniques (Imputation, Heckman, BASL), and evaluated the performance of these methods using various metrics.\n",
    "\n",
    "The next steps could involve further tuning the models, exploring other missing data techniques, and applying the model to real-world datasets for more robust conclusions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
